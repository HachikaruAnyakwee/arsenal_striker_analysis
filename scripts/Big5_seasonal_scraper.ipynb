{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hachikaruanyakwee/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory configuration\n",
    "DATA_DIR = \"data\"\n",
    "RAW_DATA_DIR = os.path.join(\"..\", DATA_DIR, \"raw\", \"Big5\")\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasons to scrape (FBref format)\n",
    "SEASONS = {\n",
    "    \"2023-2024\": \"https://fbref.com/en/comps/24/2023-2024/stats/2023-2024-Serie-A-Stats\",\n",
    "    \"2022-2023\": \"https://fbref.com/en/comps/24/2022-2023/stats/2022-2023-Serie-A-Stats\",\n",
    "    \"2021-2022\": \"https://fbref.com/en/comps/24/2021-2022/stats/2021-2022-Serie-A-Stats\",\n",
    "    \"2020-2021\": \"https://fbref.com/en/comps/24/2020-2021/stats/2020-2021-Serie-A-Stats\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape FBref stats for a given season\n",
    "def get_player_stats_section(url):\n",
    "    \"\"\"Extract the player stats table from the page\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the player stats comment section\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, str) and 'id=\"all_stats_standard\"' in text)\n",
    "        if not comments:\n",
    "            return None\n",
    "            \n",
    "        # Extract the table HTML from the comment\n",
    "        comment_soup = BeautifulSoup(comments[0], 'html.parser')\n",
    "        table = comment_soup.find('table', {'id': 'stats_standard'})\n",
    "        return str(table) if table else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract comments from HTML\n",
    "def scrape_season_stats():\n",
    "    all_seasons_data = []\n",
    "    \n",
    "    for season, url in SEASONS.items():\n",
    "        print(f\"\\nScraping {season} Serie A...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        table_html = get_player_stats_section(url)\n",
    "        if not table_html:\n",
    "            print(f\"Warning: Could not extract player stats table for {season}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Parse the HTML table\n",
    "            df = pd.read_html(StringIO(table_html))[0]\n",
    "            \n",
    "            # Clean the dataframe\n",
    "            df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df.columns]\n",
    "            df = df[df['Rk'].ne('Rk')].reset_index(drop=True)  # Remove header rows\n",
    "            df['Season'] = season\n",
    "            df['ScrapeDate'] = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            all_seasons_data.append(df)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Success: Added {len(df)} players from {season} ({elapsed:.1f}s)\")\n",
    "            \n",
    "            # Save individual season data\n",
    "            season_file = os.path.join(RAW_DATA_DIR, f'serie_a_{season.replace(\"-\", \"_\")}.csv')\n",
    "            df.to_csv(season_file, index=False)\n",
    "            print(f\"Saved {season} data to {season_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {season} data: {str(e)}\")\n",
    "            \n",
    "        time.sleep(45 + abs(hash(season)) % 30)  # Randomized delay 45-75s\n",
    "    \n",
    "    return pd.concat(all_seasons_data, ignore_index=True) if all_seasons_data else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Serie A season scraping...\n",
      "\n",
      "Scraping 2023-2024 Serie A...\n",
      "Warning: Could not extract player stats table for 2023-2024\n",
      "\n",
      "Scraping 2022-2023 Serie A...\n",
      "Warning: Could not extract player stats table for 2022-2023\n",
      "\n",
      "Scraping 2021-2022 Serie A...\n",
      "Warning: Could not extract player stats table for 2021-2022\n",
      "\n",
      "Scraping 2020-2021 Serie A...\n",
      "Warning: Could not extract player stats table for 2020-2021\n",
      "\n",
      "Scraping failed - no data collected\n",
      "\n",
      "Total execution time: 0.7 seconds\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting Serie A season scraping...\")\n",
    "    \n",
    "    combined_df = scrape_season_stats()\n",
    "    \n",
    "    if combined_df is not None:\n",
    "        output_path = os.path.join(RAW_DATA_DIR, 'serie_a_combined_seasons.csv')\n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\nSuccess! Saved combined data to {output_path}\")\n",
    "        print(f\"Total players scraped: {len(combined_df)}\")\n",
    "        \n",
    "        print(\"\\nSeason Distribution:\")\n",
    "        print(combined_df['Season'].value_counts().to_string())\n",
    "        \n",
    "        print(\"\\nSample Data (Forwards Only):\")\n",
    "        print(combined_df[combined_df['Pos'].str.contains('FW', na=False)]\n",
    "              [['Player', 'Pos', 'Squad', 'Season', 'Gls', 'Ast', 'npxG']]\n",
    "              .head(5).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nScraping failed - no data collected\")\n",
    "    \n",
    "    print(f\"\\nTotal execution time: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
